import bisect
import sys
import time
from collections import defaultdict, deque
from copy import copy, deepcopy
from dataclasses import dataclass
from functools import cmp_to_key, total_ordering
from typing import Iterator, List, Mapping, Optional, Sequence, Set, Tuple

import absl  # noqa: F401

from schedulers import BaseScheduler
from utils import EventTime
from workers import Worker, WorkerPools
from workload import (
    BatchStrategy,
    ExecutionStrategies,
    ExecutionStrategy,
    Placement,
    Placements,
    Task,
    Workload,
)
from workload.profile import WorkProfile

CAPACITY = EventTime(100, EventTime.Unit.MS)


@dataclass
class GPU:
    """Class for keeping track of the allocations and demand per-GPU."""

    id: str
    worker: Worker
    weight: float
    allocations: int = 0
    outstanding_demand: float = 0
    capacity: EventTime = CAPACITY


@total_ordering
class Model(object):
    """A representation of a model that is loaded onto a Worker.

    This instance maintains a request queue that sorts the remaining tasks by their
    deadline. The `get_feasible_placements` method returns a feasible execution strategy
    with the highest batch size that meets the earliest deadline, and removes all tasks
    that are past its deadline but couldn't be scheduled.

    Args:
        profile (`WorkProfile`): The WorkProfile of the model that provides the loading
            and execution strategies.
        worker_pools (`WorkerPools`): The `WorkerPools` object that contains the
            `Worker` instances where a model can be loaded.
    """

    @total_ordering
    class Request(object):
        """A `Request` class wraps around a `Task` and provides the `Model` with access
        to the per-request execution and loading SLOs.

        This class mirrors the `Scheduler::RequestImpl` class in Clockwork.

        Args:
            task (`Task`): The `Task` to construct a request for.
            num_strategies (`int`): The number of execution strategies that were
                made available for this `Request`. This serves as a counter to make
                sure that we accurately maintain the demand and only remove the task
                once it has been completed or removed from all queues.
        """

        def __init__(self, task: Task, num_strategies: int) -> None:
            self._task = task
            self._num_strategies = num_strategies
            # Calculate the SLO for the execution of the task.
            # The execution SLO is the time by which the task must be started for
            # execution by. In the worst case, the task is going to execute with the
            # fastest strategy, so we set the execution SLO as the time of the
            # fastest strategy.
            self._execution_slo = max(
                EventTime.zero(),
                task.deadline
                - task.release_time
                - task.profile.execution_strategies.get_fastest_strategy().runtime,
            )

            # Calculate the SLO for the loading of the profile for the task.
            # The loading SLO is the time by which the model must have begun
            # loading. In the worst case, we are going to execute the task with the
            # fastest strategy and pay a cost of the time of the fastest loading.
            self._loadweights_slo = max(
                EventTime.zero(),
                self._execution_slo
                - task.profile.loading_strategies.get_fastest_strategy().runtime,
            )

        def get_demand(self) -> Tuple[float, float]:
            """Returns the new demand generated by this function, which is set to
            the time used to execute the fastest loading and executing strategy for the
            given profile.

            Returns:
                A tuple with the `loading` demand and `execution` demand.
            """
            size = (
                self._task.profile.execution_strategies.get_fastest_strategy()
                .runtime.to(EventTime.Unit.US)
                .time
            )
            execution_demand = 0
            if self._execution_slo != EventTime.zero():
                execution_demand = (
                    size * CAPACITY.to(EventTime.Unit.US).time
                ) / self._execution_slo.to(EventTime.Unit.US).time
            load_demand = 0
            if self._loadweights_slo != EventTime.zero():
                load_demand = (
                    size * CAPACITY.to(EventTime.Unit.US).time
                ) / self._loadweights_slo.to(EventTime.Unit.US).time
            return (load_demand, execution_demand)

        @property
        def task(self) -> Task:
            return self._task

        @property
        def deadline(self) -> EventTime:
            return self._task.deadline

        @property
        def num_strategies(self) -> int:
            return self._num_strategies

        @num_strategies.setter
        def num_strategies(self, num_strategies: int) -> None:
            self._num_strategies = num_strategies

        @property
        def execution_slo(self) -> EventTime:
            """The execution SLO for the request. This is the amount of time after the
            arrival of the request that the request must be completed by."""
            return self._execution_slo

        @property
        def loadweights_slo(self) -> EventTime:
            """The loading SLO for the request. This is the amount of time after the
            arrival of the request that the model must be initiated loading for by."""
            return self._loadweights_slo

        def __repr__(self) -> str:
            return f"Request(task={self.task.unique_name}, deadline={self.deadline})"

        def __lt__(self, other: "Model.Request") -> bool:
            return self.deadline < other.deadline

        def __eq__(self, other: "Model.Request") -> bool:
            return self._task.id == other._task.id

    def __init__(self, profile: WorkProfile) -> None:
        self._tasks: Mapping[Task, Model.Request] = {}
        self._request_queues: Mapping[ExecutionStrategy, List[Model.Request]] = {
            strategy: [] for strategy in profile.execution_strategies
        }
        self._profile = profile

        # Variables that correspond to the ModelLoadTracker in Clockwork.
        self._outstanding_execution_demand = 0.0
        self._outstanding_load_demand = 0.0

        # Keep track of when the model was last used on each Worker.
        self._last_used_at_worker = defaultdict(EventTime.invalid)

    def __lt__(self, other: "Model") -> bool:
        return self.id < other.id

    def __eq__(self, other: "Model") -> bool:
        return self.id == other.id

    def __str__(self) -> str:
        return f"Model(name={self._profile.name})"

    def __repr__(self) -> str:
        return str(self)

    @property
    def id(self) -> str:
        return self._profile.id

    @property
    def profile(self) -> WorkProfile:
        return self._profile

    @property
    def outstanding_requests(self) -> int:
        """Returns the number of requests available in the queue for this `Model`."""
        return len(self._tasks)

    @property
    def earliest_deadline(self) -> EventTime:
        if len(self._tasks) == 0:
            return EventTime.invalid()
        return sorted([t.deadline for t in self._tasks])[0]

    def get_last_used_at_worker(self, worker: Worker) -> EventTime:
        """Returns the last time that this model was used on the particular Worker.

        Args:
            worker (`Worker`): The worker to get the last used time for.

        Returns:
            The last time that this model was used on the particular Worker.
        """
        return self._last_used_at_worker[worker.id]

    def get_available_execution_strategies(
        self,
        current_time: EventTime,
    ) -> ExecutionStrategies:
        """Retrieve the strategies that are available for execution given the current
        set of requests available in the model. The strategies are ordered by the
        provided function. The default ordering is by the priority defined by the slack
        between the deadline and the current time. (This follows the
        `StrategyImpl::Comparator` in Clockwork.)

        The method automatically removes requests from strategy queues that cannot meet
        the deadline for the strategy.

        Args:
            current_time (`EventTime`): The current time of the simulation.

        Returns:
            An `ExecutionStrategies` object containing the available strategies sorted
            by their priority as defined by the `key` function.
        """
        available_strategies = ExecutionStrategies()

        # We first remove the requests from the queues that cannot meet their deadlines
        # with that particular execution strategy.
        for execution_strategy, request_queue in self._request_queues.items():
            while (
                len(request_queue) > 0
                and request_queue[0].deadline
                < current_time + execution_strategy.runtime
            ):
                request = request_queue.pop(0)
                request.num_strategies -= 1
                if request.num_strategies == 0:
                    self.remove_task(request.task)

        if (
            sum(len(request_queue) for request_queue in self._request_queues.values())
            == 0
        ):
            # If there are no available requests, there should be no available
            # strategies.
            return available_strategies

        strategies = []
        for strategy, request_queue in self._request_queues.items():
            if strategy.batch_size <= len(request_queue) and (
                current_time + strategy.runtime <= request_queue[0].deadline
            ):
                priority = request_queue[0].deadline - strategy.runtime - current_time
                strategies.append((priority, -strategy.batch_size, strategy))

        # We now sort the strategies by the slack and resolve ties by the batch size.
        # The batch size is inverted so that the largest batch size is chosen first.
        # We then rely on the undocumented behavior of `ExecutionStrategies` to
        # maintain the order in which the strategies are added.
        for _, _, strategy in sorted(strategies):
            available_strategies.add_strategy(strategy)

        return available_strategies

    def add_task(self, task: Task) -> None:
        """Adds a new Task to the request queue, while keeping the request queue sorted
        by the deadline.

        If the `Task` is already available in the queue, it is not added again. If it
        is not, the `Task` is added to the queue and the demand counters are updated.

        Args:
            task (`Task`): The task to add for scheduling.

        Raises:
            A `ValueError` if the `Task`'s profile does not match the model's profile.
        """
        if task.profile != self._profile:
            raise ValueError(
                f"The task's profile ({task.profile.id}) does not match the model's "
                f"profile ({self._profile.id})."
            )
        if task not in self._tasks:
            # Create a new request and add it to the queue.
            request = Model.Request(task, num_strategies=len(self._request_queues))
            self._tasks[task] = request
            for request_queue in self._request_queues.values():
                bisect.insort(request_queue, request)

            # Update the demand counters as per the request.
            load_demand, execution_demand = request.get_demand()
            self._outstanding_execution_demand += execution_demand
            self._outstanding_load_demand += load_demand

    def remove_task(self, task: Task) -> None:
        """Removes a task from the request queue and ensures correct accounting for
        tracking information.

        We assume that as soon as a `Task` is scheduled that it's loading and executing
        phase will complete. As a result, we merge the accounting from Clockwork's
        `executing` and `completed` methods in `ModelLoadTracker`.

        Args:
            task (`Task`): The task to remove from the request queue.
        """
        if task in self._tasks:
            # Get the request and update the demand counters.
            request = self._tasks[task]
            load_demand, execution_demand = request.get_demand()
            self._outstanding_load_demand -= load_demand
            self._outstanding_execution_demand -= execution_demand

            # Remove the request from all the queues and the task from the tasks.
            for request_queue in self._request_queues.values():
                if request in request_queue:
                    request_queue.remove(request)

            del self._tasks[task]

    def get_placements(
        self,
        sim_time: EventTime,
        strategy: ExecutionStrategy,
        worker_pool_id: str,
        worker_id: str = None,
    ) -> Sequence[Placement]:
        """Returns a sequence of `Placement`s that contain the `Task`s that can be
        batched together to meet the requirements of the provided `ExecutionStrategy`.

        The method internally constructs a `BatchStrategy` object corresponding to the
        provided `ExecutionStrategy` and uses it to inform the `Worker` to batch the
        tasks in the request queue. The `Task`s are then removed from the request queue.

        Args:
            sim_time (`EventTime`): The current time of the simulation. This is used to
                inform the placement time of the `Placement` objects.
            strategy (`ExecutionStrategy`): The strategy to use for batching the tasks.
            worker_pool_id (`str`): The ID of the `WorkerPool` that the `Task`s need
                to be placed on.
            worker_id (`str`): The ID of the `Worker` that the `Task`s need to be placed
                on. If `None`, the `WorkerPool` finds appropriate `Worker`s to place the
                `Task`s on.

        Returns:
            A sequence of `Placement` objects to be returned by the scheduler.

        Raises:
            A `RuntimeError` if the number of tasks in the queue is less than the batch
            size of the provided `ExecutionStrategy`.
        """
        if len(self._request_queues[strategy]) < strategy.batch_size:
            raise RuntimeError(
                f"The number of tasks in the queue "
                f"({len(self._request_queues[strategy])}) is less than the batch "
                f"size ({strategy.batch_size})."
            )

        # Construct the `Placement` objects.
        batch_strategy = BatchStrategy(execution_strategy=strategy)
        placements = []
        tasks_to_remove = []
        for request in self._request_queues[strategy][: strategy.batch_size]:
            placement = Placement.create_task_placement(
                task=request.task,
                placement_time=sim_time,
                worker_pool_id=worker_pool_id,
                worker_id=worker_id,
                execution_strategy=batch_strategy,
            )
            placements.append(placement)
            tasks_to_remove.append(request.task)

        # Update the last used at Worker time.
        self._last_used_at_worker[worker_id] = sim_time

        # Remove all the Tasks.
        for task in tasks_to_remove:
            self.remove_task(task)

        return placements


class Models:
    """Encapsulates individual instances of `Model`s available during the simulation,
    and maintains a priority queue over the models to figure out which models need to
    be loaded and evicted.

    Args:
        models (`Sequence[Model]`): The models available during the simulation.
    """

    def __init__(
        self, models: Sequence[Model] = [], worker_pools: WorkerPools = None
    ) -> None:
        self._models = {model.id: model for model in models}
        if worker_pools is not None:
            workers = [
                worker
                for worker_pool in worker_pools.worker_pools
                for worker in worker_pool.workers
            ]
            self._workers = {
                worker.id: GPU(worker.id, worker, 1 / len(workers))
                for worker in workers
            }
        else:
            self._workers = {}

        # Construct a per-model, per-GPU allocation tracker that tracks the contribution
        # of each model to the GPU's load.
        self._model_allocations = defaultdict(lambda: 0)

        # Construct a per-model priority tracker that tracks the priority of loading
        # each model onto a GPU.
        self._model_load_priorities: Mapping[str, int] = defaultdict(lambda: 0)

    def add_model(self, model: Model) -> None:
        """Adds a new model to the set of models available during the simulation.

        Args:
            model (`Model`): The model to add.
        """
        if model.id not in self._models:
            self._models[model.id] = model

    def add_task(self, task: Task) -> None:
        """Adds a new task to the request queue of the corresponding model.

        If the model corresponding to the `Task`s profile has not been seen before, a
        new instance of `Model` is created and added to the set of available models.
        If the `Task` is already available in the queue, it is not added again.

        Args:
            task (`Task`): The task to add.
        """
        if task.profile.id not in self._models:
            self.add_model(Model(task.profile))
        self._models[task.profile.id].add_task(task)

    def refresh_priorities(self, worker_pools: WorkerPools) -> None:
        """Refreshes the priority of executing each Model on the various `Worker`s.

        Args:
            worker_pools (`WorkerPools`): The `WorkerPools` object that contains the
                instances of the `Worker`s available during the simulation.
        """
        # Ensure that we have the demand and accumulation for all the Workers.
        new_workers = [
            worker
            for worker_pool in worker_pools.worker_pools
            for worker in worker_pool.workers
            if worker.id not in self._workers
        ]
        for worker in new_workers:
            self._workers[worker.id] = GPU(
                worker.id, worker, 1 / (len(new_workers) + len(self._workers))
            )

        # The `add_task` method has already calculated the total execution and loading
        # demand for each model (d_m). We now calculate the demand allocation for each
        # model on each GPU (a_mg) as a fraction of the total demand for the model.
        for model in self._models.values():
            # Clear the outstanding allocations from this model on every Worker.
            for worker in self._workers.values():
                worker.outstanding_demand -= self._model_allocations[
                    (model.id, worker.id)
                ]
                self._model_allocations[(model.id, worker.id)] = 0

            # Calculate the total weight of all the workers that has the model loaded.
            total_weight = sum(
                [
                    worker.weight
                    for worker in self._workers.values()
                    if worker.worker.is_available(model.profile) == EventTime.zero()
                ]
            )

            # Distribute the outstanding demand according to the proportional weight
            # of each Worker, if the model was loaded on any of the Workers.
            if total_weight > 0:
                for worker in self._workers.values():
                    if worker.worker.is_available(model.profile) == EventTime.zero():
                        # Calculate the a_mg as a fraction of the total demand for the
                        # model.
                        allocation = model._outstanding_execution_demand * (
                            worker.weight / total_weight
                        )
                        self._model_allocations[(model.id, worker.id)] = allocation

                        # Keep a running sum of the total load on the GPU l_g.
                        worker.outstanding_demand += allocation

                        # Update the weight of the `Worker` to be inversely
                        # proportional to the outstanding demand on the `Worker`.
                        worker.weight = (
                            # Clockwork just assumes a default SLO capacity of 100ms.
                            worker.capacity.to(EventTime.Unit.US).time
                            / (worker.outstanding_demand + sys.float_info.epsilon)
                        )

        # Now that the per-model demands have been allocated to the GPUs, we can
        # calculate the priority of each model on each GPU.
        for model in self._models.values():
            # Calculate the weight of the GPUs where the model has been loaded.
            total_weight = sum(
                [
                    worker.weight
                    for worker in self._workers.values()
                    if worker.worker.is_available(model.profile) == EventTime.zero()
                ]
            )

            # load_priority is the p_mg in the Clockwork paper. We set it initially to
            # the total /load/ demand for the model. The weights are set based on the
            # /execution/ demand on the Worker. The core idea being that a Worker that
            # has more outstanding /execution/ demand should be given a lower weight
            # for /loading/ of new models.
            load_priority = model._outstanding_load_demand
            if total_weight > 0 and load_priority > 0:
                for worker in self._workers.values():
                    if worker.worker.is_available(model.profile) == EventTime.zero():
                        # Update the priority of the model based on the demand at the
                        # Worker. The formula to be used is
                        # d_m - ((a_mg * capacity) / lg).
                        required = model._outstanding_load_demand * (
                            worker.weight / total_weight
                        )
                        served = (
                            worker.capacity.to(EventTime.Unit.US).time * required
                        ) / (worker.outstanding_demand + sys.float_info.epsilon)
                        load_priority -= served

            # Update the priority of the model on the GPU.
            self._model_load_priorities[model.id] = load_priority

    def get_model_priority(self, worker: Worker) -> Sequence[Model]:
        """Retuns the models sorted by the priority for the given `Worker`.

        This method follows the comparison function of `CompareModelPriority` in
        ClockworkScheduler.

        Args:
            worker (`Worker`): The `Worker` to get the models for.

        Returns:
            A list of `Model`s sorted by the priority.
        """

        def comparison_function(model_a: Model, model_b: Model) -> int:
            """Compares the priority of two models on a given `Worker`."""
            model_a_is_empty = (
                abs(model_a._outstanding_load_demand) <= sys.float_info.epsilon
                and abs(model_a._outstanding_execution_demand) <= sys.float_info.epsilon
            )
            model_b_is_empty = (
                abs(model_b._outstanding_load_demand) <= sys.float_info.epsilon
                and abs(model_b._outstanding_execution_demand) <= sys.float_info.epsilon
            )
            model_a_priority = self._model_load_priorities[model_a.id]
            model_b_priority = self._model_load_priorities[model_b.id]
            model_a_last_used = model_a._last_used_at_worker[worker.id]
            model_b_last_used = model_b._last_used_at_worker[worker.id]

            if model_a_is_empty and model_b_is_empty:
                # If both models are empty, then we compare the last used time.
                return (
                    (model_a_last_used - model_b_last_used).to(EventTime.Unit.US).time
                )
            elif not model_a_is_empty and not model_b_is_empty:
                # If both models are not empty, then we compare the priority.
                if model_a_priority == model_b_priority:
                    # If the priorities are equal, then we compare the last used time.
                    return (
                        (model_a_last_used - model_b_last_used)
                        .to(EventTime.Unit.US)
                        .time
                    )
                else:
                    return model_a_priority - model_b_priority
            else:
                if model_a_priority == model_b_priority:
                    # If the priorities are equal, then we compare the last used time.
                    return (
                        (model_a_last_used - model_b_last_used)
                        .to(EventTime.Unit.US)
                        .time
                    )
                else:
                    return model_a_priority - model_b_priority

        return list(
            sorted(
                self._models.values(), key=cmp_to_key(comparison_function), reverse=True
            )
        )

    def __iter__(self) -> Iterator[Model]:
        for model in self._models.values():
            yield model

    def __getitem__(self, model_id: str) -> Model:
        return self._models[model_id]

    def __contains__(self, model_id: str) -> bool:
        return model_id in self._models


class ClockworkScheduler(BaseScheduler):
    """Implements the Clockwork infer5 scheduling algorithm.

    Args:
        runtime (`EventTime`): The runtime to return to the Simulator (in us).
            If -1, the scheduler returns the actual runtime.
        goal (`str`): The goal with which to run the Clockwork scheduler.
            `"clockwork"` corresponds to the default Clockwork implementation.
            `"least_slack"` prioritizes requests with sooner deadlines across models.
        _flags (`Optional[absl.flags]`): The runtime flags that are used to initialize
            a logger instance.
    """

    def __init__(
        self,
        runtime: EventTime = EventTime.invalid(),
        goal: str = "clockwork",
        _flags: Optional["absl.flags"] = None,
    ):
        super(ClockworkScheduler, self).__init__(
            runtime=runtime,
            enforce_deadlines=True,
            _flags=_flags,
        )
        self._goal = goal
        self._run_load = _flags.scheduler_run_load if _flags else False

        # Maintain a set of `Model` instances to keep track of the request queues and
        # construct batching strategies.
        self._models = Models()

    def start(
        self,
        start_time: EventTime,
        work_profiles: Set[WorkProfile],
        worker_pools: "WorkerPools",
    ) -> Placements:
        # Maintain an instance of `Model` for each provided `WorkProfile` to keep track
        # of the request queues and construct batching strategies.
        for work_profile in work_profiles:
            self._models.add_model(Model(profile=work_profile))
        return super(ClockworkScheduler, self).start(
            start_time, work_profiles, worker_pools
        )

    def run_admission(
        self, current_time: EventTime, tasks_to_schedule: Sequence[Task]
    ) -> Sequence[Placement]:
        """Runs admission control on the given set of tasks. This method adds the
        `Task`s to the request queue of the corresponding models and returns a
        failed `Placement` for each `Task` that is past its deadline.

        This method roughly corresponds to the `run_admission_thread` method of the
        infer5 scheduler in Clockwork [here](https://tinyurl.com/45evsxbp).

        Args:
            current_time (`EventTime`): The current time of the simulation.
            tasks_to_schedule (`Sequence[Task]`): The set of `Task`s available for
                execution.

        Returns:
            A (possibly empty) `Sequence` of `Placement` instances that represent the
            failed `Task` placements.
        """
        placements = []
        for task in tasks_to_schedule:
            if self.enforce_deadlines and (
                task.deadline
                < current_time
                + task.available_execution_strategies.get_fastest_strategy().runtime
            ):
                placements.append(Placement.create_task_cancellation(task=task))
                self._logger.debug(
                    "[%s] Task %s has a deadline of %s, which has been missed. ",
                    current_time.to(EventTime.Unit.US).time,
                    task,
                    task.deadline.to(EventTime.Unit.US).time,
                )
            else:
                self._models.add_task(task)
                self._logger.debug(
                    "[%s] Added Task %s to the request queue for Model %s.",
                    current_time.to(EventTime.Unit.US).time,
                    task.unique_name,
                    task.profile.name,
                )
        return placements

    def run_load(
        self, current_time: EventTime, worker_pools: WorkerPools
    ) -> Sequence[Placement]:
        """Runs the thread that computes what models to load and evict from each Worker.

        Args:
            current_time (`EventTime`): The current time of the simulation.
            worker_pools (`WorkerPools`): The worker pools to use for determining the
                placement of the execution strategies. This is assumed to be a shallow
                copy of the actual worker pools passed by the Simulator.

        Returns:
            A (possibly empty) `Sequence` of `Placement` instances that represent the
            loading and eviction of `WorkProfile`s from `Worker`s.
        """
        placements = []
        for worker_pool in worker_pools.worker_pools:
            for worker in worker_pool.workers:
                if len(worker.get_pending_profiles()) > 0:
                    self._logger.debug(
                        "[%s] Skipping Worker %s since it has pending profiles.",
                        current_time.to(EventTime.Unit.US).time,
                        worker,
                    )
                    continue
                models_sorted_by_priority = self._models.get_model_priority(worker)
                self._logger.debug(
                    "[%s] Received models %s for Worker %s sorted in "
                    "order of their priority.",
                    current_time.to(EventTime.Unit.US).time,
                    models_sorted_by_priority,
                    worker,
                )
                for model_to_load in models_sorted_by_priority:
                    if worker.is_available(model_to_load.profile) == EventTime.zero():
                        self._logger.debug(
                            "[%s] Skipping loading Model %s on Worker %s since "
                            "it already exists.",
                            current_time.to(EventTime.Unit.US).time,
                            model_to_load.profile.name,
                            worker,
                        )
                    elif (
                        len(
                            worker.get_compatible_strategies(
                                model_to_load.profile.loading_strategies
                            )
                        )
                        > 0
                    ):
                        self._logger.debug(
                            "[%s] The Worker can accomodate the Model %s without "
                            "any evictions.",
                            current_time.to(EventTime.Unit.US).time,
                            model_to_load.profile.name,
                        )
                        placements.append(
                            Placement.create_load_profile_placement(
                                work_profile=model_to_load.profile,
                                placement_time=current_time,
                                worker_pool_id=worker_pool.id,
                                worker_id=worker.id,
                                loading_strategy=worker.get_compatible_strategies(
                                    model_to_load.profile.loading_strategies
                                ).get_fastest_strategy(),
                            )
                        )
                        break
                    else:
                        # Find the fastest strategy that can be accommodated into an
                        # empty Worker.
                        worker_deepcopy: Worker = deepcopy(worker)
                        strategy = worker_deepcopy.get_compatible_strategies(
                            model_to_load.profile.loading_strategies
                        ).get_fastest_strategy()

                        # We iterate over the models sorted by their eviction priority
                        # and keep evicting models until the current model can be
                        # accomodated.
                        load_successful = False
                        for model_to_evict in models_sorted_by_priority[::-1]:
                            if model_to_evict == model_to_load:
                                self._logger.debug(
                                    "[%s] Skipping loading Model %s on Worker %s since "
                                    "no lower priority model can be evicted to "
                                    "accomodate it.",
                                    current_time.to(EventTime.Unit.US).time,
                                    model_to_load.profile.name,
                                    worker,
                                )
                                break
                            if (
                                worker.is_available(model_to_evict.profile)
                                == EventTime.zero()
                            ):
                                self._logger.debug(
                                    "[%s] Evicting Model %s from Worker %s.",
                                    current_time.to(EventTime.Unit.US).time,
                                    model_to_evict.profile.name,
                                    worker,
                                )
                                placements.append(
                                    Placement.create_evict_profile_placement(
                                        work_profile=model_to_evict.profile,
                                        placement_time=current_time,
                                        worker_pool_id=worker_pool.id,
                                        worker_id=worker.id,
                                    )
                                )
                                worker.evict_profile(model_to_evict.profile)
                                if worker.can_accomodate_strategy(strategy):
                                    self._logger.debug(
                                        "[%s] The Worker can now accomodate the Model "
                                        "%s with the strategy %s.",
                                        current_time.to(EventTime.Unit.US).time,
                                        model_to_load.profile.name,
                                        strategy,
                                    )
                                    placements.append(
                                        Placement.create_load_profile_placement(
                                            work_profile=model_to_load.profile,
                                            placement_time=current_time,
                                            worker_pool_id=worker_pool.id,
                                            worker_id=worker.id,
                                            loading_strategy=strategy,
                                        )
                                    )
                                    load_successful = True
                                    break

                        if not load_successful:
                            self._logger.debug(
                                "[%s] The Worker cannot accomodate the Model %s.",
                                current_time.to(EventTime.Unit.US).time,
                                model_to_load.profile.name,
                            )
                        else:
                            break
            return placements

    def run_inference(
        self, current_time: EventTime, worker_pools: WorkerPools
    ) -> Sequence[Placement]:
        """Runs the inference algorithm for all of the `Worker`s in the `WorkerPools`
        and returns the `Placement` objects that represent the execution of requests
        with the given strategy on the given `Worker`.

        Args:
            current_time (EventTime): The current simulation time.
            worker_pools (WorkerPools): The `WorkerPools` to run inference on.

        Returns:
            A `Sequence` of `Placement` objects that represent the strategy of execution
            of the `Task`s on the available `Worker`s.
        """
        placements = []
        for worker_pool in worker_pools.worker_pools:
            for worker in worker_pool.workers:
                # Retrieve all of the available execution strategies for all models.
                execution_strategies_queue: Sequence[
                    Tuple[Model, ExecutionStrategies]
                ] = deque()
                for model in self._models:
                    model_strategies = model.get_available_execution_strategies(
                        current_time=current_time
                    )
                    if len(model_strategies) > 0:
                        execution_strategies_queue.append((model, model_strategies))

                if self._goal == "least_slack":
                    # Sort by next deadline.
                    execution_strategies_queue = deque(
                        sorted(
                            execution_strategies_queue,
                            key=lambda t: t[0].earliest_deadline,
                        )
                    )

                # Find the set of placements that this `Worker` can accomodate.
                while len(execution_strategies_queue) > 0:
                    model, model_strategies = execution_strategies_queue.popleft()

                    # If the model is not loaded on this `Worker`, we can skip it.
                    if worker.is_available(model.profile) != EventTime.zero():
                        self._logger.debug(
                            "[%s] Skipping Worker %s since the Model %s (%s) is not "
                            "loaded. The model will be available in %s.",
                            current_time.to(EventTime.Unit.US).time,
                            worker,
                            model.profile.name,
                            model.profile.id,
                            worker.is_available(model.profile),
                        )
                        continue

                    # If the model is loaded, we iterate over the strategies and find
                    # the first one that can be accomodated by the `Worker`.
                    model_placed = False
                    for execution_strategy in worker.get_compatible_strategies(
                        model_strategies
                    ):
                        task_placements: Sequence[Placement] = model.get_placements(
                            sim_time=current_time,
                            strategy=execution_strategy,
                            worker_pool_id=worker_pool.id,
                            worker_id=worker.id,
                        )
                        for placement in task_placements:
                            placements.append(placement)
                            worker.place_task(
                                task=placement.task,
                                execution_strategy=placement.execution_strategy,
                            )
                            self._logger.debug(
                                "[%s] Requesting to execute Task %s with Model %s "
                                "on Worker %s with the strategy %s.",
                                current_time.to(EventTime.Unit.US).time,
                                placement.task,
                                model.profile,
                                worker,
                                placement.execution_strategy,
                            )
                        model_placed = True
                        break

                    # If some strategy for the model was placed, we add new strategies
                    # to the queue for the same model at the end.
                    if model_placed:
                        new_model_strategies = model.get_available_execution_strategies(
                            current_time=current_time
                        )
                        if len(new_model_strategies) > 0:
                            execution_strategies_queue.append(
                                (model, new_model_strategies)
                            )
                            if self._goal == "least_slack":
                                # Sort by next deadline.
                                execution_strategies_queue = deque(
                                    sorted(
                                        execution_strategies_queue,
                                        key=lambda t: t[0].earliest_deadline,
                                    )
                                )
                    else:
                        self._logger.debug(
                            "[%s] No valid placements were found for Model %s.",
                            current_time.to(EventTime.Unit.US).time,
                            model.profile.name,
                        )
        return placements

    def schedule(
        self, sim_time: EventTime, workload: Workload, worker_pools: WorkerPools
    ) -> Placements:
        # Retrieve the schedulable tasks from the Workload.
        tasks_to_be_scheduled = workload.get_schedulable_tasks(
            time=sim_time,
            lookahead=self.lookahead,
            preemption=self.preemptive,
            retract_schedules=self.retract_schedules,
            worker_pools=worker_pools,
            policy=self.policy,
            branch_prediction_accuracy=self.branch_prediction_accuracy,
        )

        # Create a virtual WorkerPool set to try scheduling decisions on.
        schedulable_worker_pools = copy(worker_pools)
        num_workers = sum(
            len(worker_pool.workers)
            for worker_pool in schedulable_worker_pools.worker_pools
        )
        self._logger.debug(
            f"[{sim_time.time}] The scheduler received {len(tasks_to_be_scheduled)} "
            f"tasks for scheduling across {num_workers} workers. "
            f"These tasks were: {[task.unique_name for task in tasks_to_be_scheduled]}."
        )

        scheduler_start_time = time.time()
        placements = []

        # Run admission control on the `Task`s available for execution.
        failed_admission_control_tasks = self.run_admission(
            current_time=sim_time, tasks_to_schedule=tasks_to_be_scheduled
        )
        if len(failed_admission_control_tasks) > 0:
            self._logger.debug(
                "[%s] A total of %d tasks (out of %d) failed admission control.",
                sim_time.to(EventTime.Unit.US).time,
                len(failed_admission_control_tasks),
                len(tasks_to_be_scheduled),
            )
            placements.extend(failed_admission_control_tasks)

        # Run model loading algorithm on the `Worker`s available for placement.
        if self._run_load:
            # Now that the requests are available in the queue, we can refresh the
            # priorities of the models on each Worker.
            self._models.refresh_priorities(worker_pools=schedulable_worker_pools)

            profile_placements = self.run_load(
                current_time=sim_time, worker_pools=schedulable_worker_pools
            )
            if len(profile_placements) > 0:
                self._logger.debug(
                    "[%s] A total of %d model loading/evictions will be done on the "
                    "workers.",
                    sim_time.to(EventTime.Unit.US).time,
                    len(profile_placements),
                )
                placements.extend(profile_placements)

        # Run the inference algorithm to decide placements of `Task`s on `Worker`s.
        task_placements = self.run_inference(
            current_time=sim_time, worker_pools=schedulable_worker_pools
        )
        if len(task_placements) > 0:
            self._logger.debug(
                "[%s] A total of %d tasks will be placed. These tasks are: [%s].",
                sim_time.to(EventTime.Unit.US).time,
                len(task_placements),
                ", ".join(
                    [placement.task.unique_name for placement in task_placements]
                ),
            )
            placements.extend(task_placements)

        scheduler_end_time = time.time()
        scheduler_runtime = EventTime(
            int((scheduler_end_time - scheduler_start_time) * 1e6), EventTime.Unit.US
        )
        self._logger.debug(
            f"[{sim_time.time}] The runtime of the scheduler was: {scheduler_runtime}."
        )
        runtime = (
            scheduler_runtime
            if self.runtime == EventTime(-1, EventTime.Unit.US)
            else self.runtime
        )
        return Placements(
            runtime=runtime, true_runtime=scheduler_runtime, placements=placements
        )
